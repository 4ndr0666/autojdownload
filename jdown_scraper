#! /bin/bash

# define colors and escape sequence
lightgreen='\033[1;32m'
lightred='\033[1;31m'
orange='\033[0;33m'
cyan='\033[0;36m'
wht='\033[0m'
nc='\033[0J'


# scrape_urls() function scrapes urls from given web page using lynx, cut, grep, awk, sed, sort, uniq tools.
# it can scrape domains, relative urls and all urls.
function scrape_urls() {
    # remove the previous output file if exists
    rm -f urls_to_check.txt
    
    # loop through all files in the specified directory
    for f in "$1"/*; do
        # check if the file is a text file
        if file -b "$f" | grep -q "text"; then
            # extract the URLs and write them to the output file
            grep -Eo "(http|https)://[a-zA-Z0-9./?=_-]*" "$f" >> urls_to_check.txt
        fi
    done
}


# url_check() function checks if scraped url are working or not
# curl is used to check the response of the url
function url_check() {
    # check if the input file exists
    if [ ! -f "urls_to_check.txt" ]; then
        echo "Input file not found: urls_to_check.txt"
        exit 1
    fi

    # counters for various types of link
    redirected=0
    success=0
    error=0
    total=0

    # variables for rich print
    if [ "$1" == "-r" ]; then
        lnktp="relative url(s)"
        append="$2/"
    elif [ "$1" == "-d" ]; then
        lnktp="domain(s)"
        append=""
    else
        lnktp="url(s)"
        append=""
    fi

    # read each line from the input file and perform HTTP request to find status
    while read -r line; do
        aline="$append$line"
        sc=$(curl -Is "$aline" | head -n 1 )
        read -ra code <<< "$sc"
        total=$(expr $total + 1)
        case "${code[1]}" in
            1[0-9][0-9]) echo -e "${cyan}$line${nc}" # informational response
                ;;
            2[0-9][0-9]) echo -e "${lightgreen}$line${nc}" # successful response
                if /usr/bin/java -jar "/path/to/JDownloader2/JDownloader.jar" "callAPI" "ping" "http://127.0.0.1:9666"; then
                    echo "linklist:$aline" | /usr/bin/java -jar "/path/to/JDownloader2/JDownloader.jar" "callAPI" "addLinks" "http://127.0.0.1:9666"
                fi
                success=$(expr $success + 1)
                ;;
            3[0-9][0-9]) echo -e "${orange}$line${nc}" # redirection response
                redirected=$(expr $redirected + 1)
                ;;
            4[0-9][0-9]) echo -e "${lightred}$line${nc}" # client response
                error=$(expr "$error" + 1)
                ;;
            5[0-9][0-9]) echo -e "${lightred}$line${nc}" # server response
                error=$(expr "$error" + 1)
                ;;
        esac
        echo -ne "${wht}[found ${cyan}$total ${wht}$lnktp: ${lightgreen}$success OK ${wht}| ${orange}$redirected Redirected ${wht}| ${lightred}$error Broken${wht}]${nc}\r" | /path/to/jdownloader_watch.sh # progress 
    done < <(scrape_urls "$3") # command substitution

    if [ $total -eq 0 -a $# -ne 0 ]; then
        echo -e "${wht}[found ${cyan}$total ${wht}$lnktp: ${lightgreen}$success OK ${wht}| ${orange}$redirected Redirected ${wht}| ${lightred}$error Broken${wht}]${nc}" # print in case no urls are found
    fi
}

url_check "$1" "$2" "$3"

exit 0

